{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Quora Question Pairs - Basic Transfer learning with BERT\n\nIn this notebook I present a basic workflow to transfer learning from a general language model (BERT) to a specific task of called paraphrasing: identifiy if two sentences have same meaning.","metadata":{}},{"cell_type":"code","source":"import transformers\nimport torch\nfrom transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification, Trainer\nfrom transformers import TrainingArguments\nfrom datasets import ClassLabel, Value\nfrom transformers import DataCollatorWithPadding\nimport numpy as np\nfrom datasets import load_metric","metadata":{"execution":{"iopub.status.busy":"2024-08-15T13:12:15.559634Z","iopub.execute_input":"2024-08-15T13:12:15.560218Z","iopub.status.idle":"2024-08-15T13:12:15.565285Z","shell.execute_reply.started":"2024-08-15T13:12:15.560175Z","shell.execute_reply":"2024-08-15T13:12:15.564484Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"For this task, I used Hugging Face library, with make our job really straightforward. To save GPU budged in this demo, I choose a basic transformer model (Bert-base-uncased)\n\nThe classes `AutoTokenizer` and `AutoModelForSequenceClassification` have the workflow steps to preprocess the text and model architecture respectivelly. Note since we are using `AutoModelForSequenceClassification` and BERT was pre-trained for a Language Model task, a warning is shown saying the last layer of our model is uninitialized.","metadata":{}},{"cell_type":"code","source":"\ncheckpoint = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T13:12:20.284034Z","iopub.execute_input":"2024-08-15T13:12:20.284346Z","iopub.status.idle":"2024-08-15T13:12:35.251773Z","shell.execute_reply.started":"2024-08-15T13:12:20.284305Z","shell.execute_reply":"2024-08-15T13:12:35.251135Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afa89ce7db254c3fbe82a21dcd169a7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b45117067804f109a6134ed65466d19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d2327d012ef4f888402523e1e65159f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6246fabff7504abc8f93fddee7ade3f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2674f5f083be44a9b8c0281bc47def54"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Here we say pytorch to use GPU if available.","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.to(device)\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-08-15T13:12:46.934592Z","iopub.execute_input":"2024-08-15T13:12:46.934890Z","iopub.status.idle":"2024-08-15T13:12:52.251727Z","shell.execute_reply.started":"2024-08-15T13:12:46.934855Z","shell.execute_reply":"2024-08-15T13:12:52.250770Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"markdown","source":"Now, we use `load_dataset` method to load the dataset from Hugging Face Hub (a models, dataset and other resources repository): https://huggingface.co/datasets/quora","metadata":{}},{"cell_type":"markdown","source":"## Load and preprocess our data","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\nraw_datasets = load_dataset(\"quora\")\nraw_datasets","metadata":{"execution":{"iopub.status.busy":"2024-08-15T13:12:59.294704Z","iopub.execute_input":"2024-08-15T13:12:59.295266Z","iopub.status.idle":"2024-08-15T13:13:36.568773Z","shell.execute_reply.started":"2024-08-15T13:12:59.295224Z","shell.execute_reply":"2024-08-15T13:13:36.567920Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42c7bd5b8acb4fa9881c6f7bc57e83f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/559 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13b1dcb8d8214686ae63eaa9c5ca2571"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset quora/default (download: 55.48 MiB, generated: 55.46 MiB, post-processed: Unknown size, total: 110.94 MiB) to /root/.cache/huggingface/datasets/quora/default/0.0.0/36ba4cd42107f051a158016f1bea6ae3f4685c5df843529108a54e42d86c1e04...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7ee1ef0e3ef4f29bb612c8a49974b58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/58.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd3dc6eec1aa41aa84b520f2950ede63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f5da49081794a408e0ff9f32b2e0b35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset quora downloaded and prepared to /root/.cache/huggingface/datasets/quora/default/0.0.0/36ba4cd42107f051a158016f1bea6ae3f4685c5df843529108a54e42d86c1e04. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36fd038db8744d10b5236372d0393384"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['questions', 'is_duplicate'],\n        num_rows: 404290\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"Above we can see the number of examples and features names.","metadata":{}},{"cell_type":"code","source":"raw_datasets['train'].features","metadata":{"execution":{"iopub.status.busy":"2024-08-15T13:13:44.904896Z","iopub.execute_input":"2024-08-15T13:13:44.905214Z","iopub.status.idle":"2024-08-15T13:13:44.912248Z","shell.execute_reply.started":"2024-08-15T13:13:44.905176Z","shell.execute_reply":"2024-08-15T13:13:44.911403Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'questions': Sequence(feature={'id': Value(dtype='int32', id=None), 'text': Value(dtype='string', id=None)}, length=-1, id=None),\n 'is_duplicate': Value(dtype='bool', id=None)}"},"metadata":{}}]},{"cell_type":"markdown","source":"Here there are two example of sentence pairs and their labels (if is duplicated or not).","metadata":{}},{"cell_type":"code","source":"raw_datasets['train'][0:2]","metadata":{"execution":{"iopub.status.busy":"2024-08-15T13:13:54.823162Z","iopub.execute_input":"2024-08-15T13:13:54.823473Z","iopub.status.idle":"2024-08-15T13:13:54.831372Z","shell.execute_reply.started":"2024-08-15T13:13:54.823439Z","shell.execute_reply":"2024-08-15T13:13:54.830455Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"{'questions': [{'id': [1, 2],\n   'text': ['What is the step by step guide to invest in share market in india?',\n    'What is the step by step guide to invest in share market?']},\n  {'id': [3, 4],\n   'text': ['What is the story of Kohinoor (Koh-i-Noor) Diamond?',\n    'What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?']}],\n 'is_duplicate': [False, False]}"},"metadata":{}}]},{"cell_type":"markdown","source":"Since `AutoTokenizer` class expects as input a list of sentence pairs, we need to process our dataset first.","metadata":{}},{"cell_type":"code","source":"def tokenize_function(example):\n    questions = example['questions']\n    t1 = []\n    t2 = []\n    for t in questions:\n        t1.append(t['text'][0])\n        t2.append(t['text'][1])\n    return tokenizer(t1, t2, truncation=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T13:13:58.178441Z","iopub.execute_input":"2024-08-15T13:13:58.179261Z","iopub.status.idle":"2024-08-15T13:13:58.185172Z","shell.execute_reply.started":"2024-08-15T13:13:58.179219Z","shell.execute_reply":"2024-08-15T13:13:58.184217Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets = raw_datasets['train'].map(tokenize_function, batched=True)\ntokenized_datasets","metadata":{"execution":{"iopub.status.busy":"2024-08-15T13:14:31.195792Z","iopub.execute_input":"2024-08-15T13:14:31.196079Z","iopub.status.idle":"2024-08-15T13:15:14.596931Z","shell.execute_reply.started":"2024-08-15T13:14:31.196030Z","shell.execute_reply":"2024-08-15T13:15:14.596104Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/405 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7182d0bed8d451ca3a75afd2cde6f27"}},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['attention_mask', 'input_ids', 'is_duplicate', 'questions', 'token_type_ids'],\n    num_rows: 404290\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"Here we drop original columns, cast the boolean type to `ClassLabel`, rename `is_duplicate` to `labels` and split data into train (80%) and test (20%)","metadata":{}},{"cell_type":"code","source":"new_features = tokenized_datasets.features.copy()\nnew_features[\"is_duplicate\"] = ClassLabel(num_classes=2, names=['not_duplicate', 'duplicate'], names_file=None, id=None)\ntokenized_datasets = tokenized_datasets.cast(new_features)\ntokenized_datasets = tokenized_datasets.remove_columns('questions').rename_column('is_duplicate', 'labels')\ntokenized_datasets = tokenized_datasets.train_test_split(test_size=0.2)\ntokenized_datasets","metadata":{"execution":{"iopub.status.busy":"2024-08-15T13:15:17.898215Z","iopub.execute_input":"2024-08-15T13:15:17.898929Z","iopub.status.idle":"2024-08-15T13:15:54.326172Z","shell.execute_reply.started":"2024-08-15T13:15:17.898889Z","shell.execute_reply":"2024-08-15T13:15:54.325348Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Casting the dataset:   0%|          | 0/41 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d02dfd053197436c8bcf6ab68e682030"}},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['attention_mask', 'input_ids', 'labels', 'token_type_ids'],\n        num_rows: 323432\n    })\n    test: Dataset({\n        features: ['attention_mask', 'input_ids', 'labels', 'token_type_ids'],\n        num_rows: 80858\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"Almost there! Our function `tokenize_function` used only the option truncate=True. This causes a problem, because the transformer model expects every input to have the same lenght (**Padding**). But doing this for all records at once demands to fit our dataset into memory. This is slow and resource consuming.\nSo we'll use a dynamic padding during the collation process. Collation is what pytorch do to put examples together into batchs.\n\nFor our convenience, transformers already have a class for that:","metadata":{}},{"cell_type":"code","source":"data_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T13:15:57.708678Z","iopub.execute_input":"2024-08-15T13:15:57.709268Z","iopub.status.idle":"2024-08-15T13:15:57.713490Z","shell.execute_reply.started":"2024-08-15T13:15:57.709225Z","shell.execute_reply":"2024-08-15T13:15:57.712566Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Now whe can sample few examples to see if our collator works.","metadata":{}},{"cell_type":"code","source":"samples = tokenized_datasets['train'][:8]\nsamples = {k: v for k, v in samples.items()}\nbatch = data_collator(samples)\nbatch = batch.to(device)\n{k: v.shape for k, v in batch.items()}","metadata":{"execution":{"iopub.status.busy":"2024-08-15T13:16:07.629289Z","iopub.execute_input":"2024-08-15T13:16:07.629625Z","iopub.status.idle":"2024-08-15T13:16:07.641583Z","shell.execute_reply.started":"2024-08-15T13:16:07.629589Z","shell.execute_reply":"2024-08-15T13:16:07.640498Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"{'attention_mask': torch.Size([8, 55]),\n 'input_ids': torch.Size([8, 55]),\n 'labels': torch.Size([8]),\n 'token_type_ids': torch.Size([8, 55])}"},"metadata":{}}]},{"cell_type":"markdown","source":"## Fine tuning our BERT model\n\nFinally, we're ready to heat our processors to do the job, using the `Trainer` API. \nWe need to provide a function to compute scores, since transformers returns logits and we need to calculate accuracy and F1 score by hand. Also, we pass some parameters to `TrainingArguments` class before create our `Trainer`","metadata":{}},{"cell_type":"code","source":"\ndef compute_metrics(eval_preds):\n    metric = load_metric(\"glue\", \"mrpc\")\n    logits, labels = eval_preds\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T13:16:13.786413Z","iopub.execute_input":"2024-08-15T13:16:13.786988Z","iopub.status.idle":"2024-08-15T13:16:13.792561Z","shell.execute_reply.started":"2024-08-15T13:16:13.786947Z","shell.execute_reply":"2024-08-15T13:16:13.791454Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\"./quora-saved-model\", evaluation_strategy=\"epoch\", save_strategy='no', \n                                  report_to='none', num_train_epochs=3, \n                                  per_device_train_batch_size=32,\n                                  per_device_eval_batch_size=32)\ntrainer = Trainer(\n    model,\n    training_args,\n    train_dataset=tokenized_datasets['train'],\n    eval_dataset=tokenized_datasets['test'],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T13:16:17.573638Z","iopub.execute_input":"2024-08-15T13:16:17.573951Z","iopub.status.idle":"2024-08-15T13:16:17.587594Z","shell.execute_reply.started":"2024-08-15T13:16:17.573913Z","shell.execute_reply":"2024-08-15T13:16:17.586611Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-08-15T13:16:23.281903Z","iopub.execute_input":"2024-08-15T13:16:23.282565Z","iopub.status.idle":"2024-08-15T15:30:23.635012Z","shell.execute_reply.started":"2024-08-15T13:16:23.282522Z","shell.execute_reply":"2024-08-15T15:30:23.634181Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"***** Running training *****\n  Num examples = 323432\n  Num Epochs = 3\n  Instantaneous batch size per device = 32\n  Total train batch size (w. parallel, distributed & accumulation) = 32\n  Gradient Accumulation steps = 1\n  Total optimization steps = 30324\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='30324' max='30324' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [30324/30324 2:13:59, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.259600</td>\n      <td>0.251683</td>\n      <td>0.896040</td>\n      <td>0.859253</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.167600</td>\n      <td>0.249931</td>\n      <td>0.907282</td>\n      <td>0.878782</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.085900</td>\n      <td>0.325111</td>\n      <td>0.910287</td>\n      <td>0.879313</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 80858\n  Batch size = 32\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.86k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5b8fbfadd9d4efcad6a834751f5c193"}},"metadata":{}},{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 80858\n  Batch size = 32\n***** Running Evaluation *****\n  Num examples = 80858\n  Batch size = 32\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=30324, training_loss=0.19235535508299106, metrics={'train_runtime': 8040.3046, 'train_samples_per_second': 120.679, 'train_steps_per_second': 3.771, 'total_flos': 3.403727534333136e+16, 'train_loss': 0.19235535508299106, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"eval_results = trainer.evaluate()\nprint(eval_results)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T15:31:14.486314Z","iopub.execute_input":"2024-08-15T15:31:14.486700Z","iopub.status.idle":"2024-08-15T15:34:36.898837Z","shell.execute_reply.started":"2024-08-15T15:31:14.486659Z","shell.execute_reply":"2024-08-15T15:34:36.897964Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 80858\n  Batch size = 32\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2527' max='2527' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2527/2527 03:21]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.325111448764801, 'eval_accuracy': 0.9102871701006703, 'eval_f1': 0.8793132133231292, 'eval_runtime': 202.3987, 'eval_samples_per_second': 399.499, 'eval_steps_per_second': 12.485, 'epoch': 3.0}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"With only 3 epochs we achieved an accuracy and f1-score near 90%, using a basic model that disconsider letter cases.\n\n## 3. Test our model\n\nLastly, we can play with our self-created pair of sentences and test our model.","metadata":{}},{"cell_type":"code","source":"tokens = tokenizer([\n    ['How can I be successful in Kaggle Competitions?', 'How can I be successful in life?'],\n    ['What is the best place to eat a pizza in Italy?','What is the best restaurant in Italy?'],\n    ['What are the good courses to learn pytorch?','Are there good courses to learn pytorch?']],\n    truncation=True, padding=True, return_tensors='pt')\n\ntokens.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T15:34:49.465479Z","iopub.execute_input":"2024-08-15T15:34:49.466165Z","iopub.status.idle":"2024-08-15T15:34:49.481051Z","shell.execute_reply.started":"2024-08-15T15:34:49.466129Z","shell.execute_reply":"2024-08-15T15:34:49.480158Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[  101,  2129,  2064,  1045,  2022,  3144,  1999, 10556, 24679,  6479,\n          1029,   102,  2129,  2064,  1045,  2022,  3144,  1999,  2166,  1029,\n           102,     0,     0,     0,     0,     0],\n        [  101,  2054,  2003,  1996,  2190,  2173,  2000,  4521,  1037, 10733,\n          1999,  3304,  1029,   102,  2054,  2003,  1996,  2190,  4825,  1999,\n          3304,  1029,   102,     0,     0,     0],\n        [  101,  2054,  2024,  1996,  2204,  5352,  2000,  4553,  1052, 22123,\n          2953,  2818,  1029,   102,  2024,  2045,  2204,  5352,  2000,  4553,\n          1052, 22123,  2953,  2818,  1029,   102]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n         0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n         0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n         0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n         0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1]], device='cuda:0')}"},"metadata":{}}]},{"cell_type":"code","source":"logits = model(**tokens).logits\nlogits = logits.cpu().detach().numpy()\npreds = np.argmax(logits, axis=-1)\npreds","metadata":{"execution":{"iopub.status.busy":"2024-08-15T15:35:57.782117Z","iopub.execute_input":"2024-08-15T15:35:57.782893Z","iopub.status.idle":"2024-08-15T15:35:57.806410Z","shell.execute_reply.started":"2024-08-15T15:35:57.782856Z","shell.execute_reply":"2024-08-15T15:35:57.805590Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"array([0, 0, 1])"},"metadata":{}}]},{"cell_type":"code","source":"model.save_pretrained('/kaggle/working/quora-saved-model')\ntokenizer.save_pretrained('/kaggle/working/quora-saved-model')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-15T15:37:11.179002Z","iopub.execute_input":"2024-08-15T15:37:11.179819Z","iopub.status.idle":"2024-08-15T15:37:12.071530Z","shell.execute_reply.started":"2024-08-15T15:37:11.179783Z","shell.execute_reply":"2024-08-15T15:37:12.070627Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"Configuration saved in /kaggle/working/quora-saved-model/config.json\nModel weights saved in /kaggle/working/quora-saved-model/pytorch_model.bin\ntokenizer config file saved in /kaggle/working/quora-saved-model/tokenizer_config.json\nSpecial tokens file saved in /kaggle/working/quora-saved-model/special_tokens_map.json\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/quora-saved-model/tokenizer_config.json',\n '/kaggle/working/quora-saved-model/special_tokens_map.json',\n '/kaggle/working/quora-saved-model/vocab.txt',\n '/kaggle/working/quora-saved-model/added_tokens.json',\n '/kaggle/working/quora-saved-model/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"# Saving the model\nmodel.save_pretrained('/kaggle/working/quora-saved-model')\ntokenizer.save_pretrained('/kaggle/working/quora-saved-model')\n\n# Loading the model\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nmodel = AutoModelForSequenceClassification.from_pretrained('/kaggle/working/quora-saved-model')\ntokenizer = AutoTokenizer.from_pretrained('/kaggle/working/quora-saved-model')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-15T15:37:34.249675Z","iopub.execute_input":"2024-08-15T15:37:34.249993Z","iopub.status.idle":"2024-08-15T15:37:36.867111Z","shell.execute_reply.started":"2024-08-15T15:37:34.249955Z","shell.execute_reply":"2024-08-15T15:37:36.866152Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"Configuration saved in /kaggle/working/quora-saved-model/config.json\nModel weights saved in /kaggle/working/quora-saved-model/pytorch_model.bin\ntokenizer config file saved in /kaggle/working/quora-saved-model/tokenizer_config.json\nSpecial tokens file saved in /kaggle/working/quora-saved-model/special_tokens_map.json\nloading configuration file /kaggle/working/quora-saved-model/config.json\nModel config BertConfig {\n  \"_name_or_path\": \"bert-base-uncased\",\n  \"architectures\": [\n    \"BertForSequenceClassification\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"problem_type\": \"single_label_classification\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.12.5\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading weights file /kaggle/working/quora-saved-model/pytorch_model.bin\nAll model checkpoint weights were used when initializing BertForSequenceClassification.\n\nAll the weights of BertForSequenceClassification were initialized from the model checkpoint at /kaggle/working/quora-saved-model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\nDidn't find file /kaggle/working/quora-saved-model/added_tokens.json. We won't load it.\nloading file /kaggle/working/quora-saved-model/vocab.txt\nloading file /kaggle/working/quora-saved-model/tokenizer.json\nloading file None\nloading file /kaggle/working/quora-saved-model/special_tokens_map.json\nloading file /kaggle/working/quora-saved-model/tokenizer_config.json\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}